{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz\n",
    "\n",
    "dot = graphviz.Digraph(\"test\", format = \"svg\", graph_attr = {'rankdir': 'LR'})\n",
    "dot.node(\"thing 1\", \"hello world!\")\n",
    "dot.node(\"thing 2\", \"my name is paul!\")\n",
    "dot.edge(\"thing 1\", \"thing 2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.43.0 (0)\n",
       " -->\n",
       "<!-- Title: test Pages: 1 -->\n",
       "<svg width=\"359pt\" height=\"44pt\"\n",
       " viewBox=\"0.00 0.00 358.57 44.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 40)\">\n",
       "<title>test</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-40 354.57,-40 354.57,4 -4,4\"/>\n",
       "<!-- thing 1 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>thing 1</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"66.29\" cy=\"-18\" rx=\"66.09\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"66.29\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">hello world!</text>\n",
       "</g>\n",
       "<!-- thing 2 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>thing 2</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"259.58\" cy=\"-18\" rx=\"90.98\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"259.58\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">my name is paul!</text>\n",
       "</g>\n",
       "<!-- thing 1&#45;&gt;thing 2 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>thing 1&#45;&gt;thing 2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M132.59,-18C140.91,-18 149.58,-18 158.3,-18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"158.54,-21.5 168.54,-18 158.54,-14.5 158.54,-21.5\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x7f06e98cc8e0>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input file size': '842647B', 'token count': 230417, 'tokenized data volume': '460834B', 'estimated minimum file size': '353395.6045227051B', 'distinct token count': 11196, 'top elements total / proportion of tokens': (183437, 0.7961087940559941), 'bottom elements total  / proportion of tokens': (46148, 0.2002803612580669), 'top elements distinct count / proportion of distinct tokens': (672, 0.06002143622722401), 'bottom elements count / proportion of distinct tokens': (10498, 0.9376563058235085)}\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import tiktoken\n",
    "import functools\n",
    "\n",
    "def convertINT16To2Chr(input): #could be replaced with bit shifting.  probably faster w/ bit shifts?\n",
    "    output = \"\"\n",
    "    buffer1 = int(input / 256)\n",
    "    buffer2 = input - (buffer1 * 256)\n",
    "    output = [chr(buffer1), chr(buffer2)]\n",
    "    return(output)\n",
    "\n",
    "text = open(\"Dracula.txt\", 'r')\n",
    "testOutput = open(\"compressionTest.txt\", 'w')\n",
    "tokenizer = tiktoken.get_encoding('gpt2')\n",
    "singleStringText = text.read()\n",
    "buffer = singleStringText.split(\" \")\n",
    "splits = list(map(lambda x: ' '.join(buffer[x:x+100]), range(0, len(buffer), 100)))\n",
    "tokenSets = list(map(lambda x: tokenizer.encode(x), splits))\n",
    "allTokens = list(functools.reduce(lambda x, y: x + y, tokenSets, []))\n",
    "inputFileSize = len(singleStringText)\n",
    "tokenCount = len(allTokens)\n",
    "dataVolume = tokenCount * 2\n",
    "estimatedMinFileSize = tokenCount * (tokenizer.n_vocab / (2**16)) * 2\n",
    "frequencies = dict(collections.Counter(allTokens))\n",
    "distinctTokenCount = len(frequencies)\n",
    "commonElements = dict(filter(lambda x: x[1] > 32, frequencies.items()))\n",
    "uncommonElements = dict(filter(lambda x: x[1] < 32, frequencies.items()))\n",
    "topElementsSum = functools.reduce(lambda x, y: x + y[1], commonElements.items(), 0)\n",
    "bottomElementsSum = functools.reduce(lambda x, y: x + y[1], uncommonElements.items(), 0)\n",
    "tokenConversionBuffer = list(map(lambda x: convertINT16To2Chr(x), allTokens))\n",
    "convertedTokens = list(functools.reduce(lambda x, y: x + [y[0], y[1]], tokenConversionBuffer))\n",
    "\n",
    "topElementsCount = len(commonElements)\n",
    "bottomElementsCount = len(uncommonElements)\n",
    "topElementsSumProp = topElementsSum / tokenCount\n",
    "bottomElementsSumProp = bottomElementsSum / tokenCount\n",
    "topElementsCountProp = topElementsCount / distinctTokenCount\n",
    "bottomElementsCountProp = bottomElementsCount / distinctTokenCount\n",
    "\n",
    "stats = {\n",
    "    'input file size': str(inputFileSize) + \"B\",\n",
    "    'token count': tokenCount,\n",
    "    'tokenized data volume': str(dataVolume) + \"B\",\n",
    "    'estimated minimum file size': str(estimatedMinFileSize) + \"B\",\n",
    "    'distinct token count': distinctTokenCount,\n",
    "    'top elements total / proportion of tokens': (topElementsSum, topElementsSumProp),\n",
    "    'bottom elements total  / proportion of tokens': (bottomElementsSum, bottomElementsSumProp),\n",
    "    'top elements distinct count / proportion of distinct tokens': (topElementsCount, topElementsCountProp),\n",
    "    'bottom elements count / proportion of distinct tokens': (bottomElementsCount, bottomElementsCountProp)\n",
    "}\n",
    "\n",
    "print(stats)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 230\n",
      "\t æ\n"
     ]
    }
   ],
   "source": [
    "x = 2534\n",
    "y = int(x / 256)\n",
    "a = y * 256\n",
    "b = x - a\n",
    "print(y, b)\n",
    "print(chr(y), chr(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u0000\n",
      "\u0001\n",
      "\u0002\n",
      "\u0003\n",
      "\u0004\n",
      "\u0005\n",
      "\u0006\n",
      "\u0007\n",
      "\b\n",
      "\t\n",
      "\n",
      "\n",
      "\u000b\n",
      "\f\n",
      "\n",
      "\u000e\n",
      "\u000f\n",
      "\u0010\n",
      "\u0011\n",
      "\u0012\n",
      "\u0013\n",
      "\u0014\n",
      "\u0015\n",
      "\u0016\n",
      "\u0017\n",
      "\u0018\n",
      "\u0019\n",
      "\u001a\n",
      "\u001b\n",
      "\u001c\n",
      "\u001d\n",
      "\u001e\n",
      "\u001f\n",
      " \n",
      "!\n",
      "\"\n",
      "#\n",
      "$\n",
      "%\n",
      "&\n",
      "'\n",
      "(\n",
      ")\n",
      "*\n",
      "+\n",
      ",\n",
      "-\n",
      ".\n",
      "/\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      ":\n",
      ";\n",
      "<\n",
      "=\n",
      ">\n",
      "?\n",
      "@\n",
      "A\n",
      "B\n",
      "C\n",
      "D\n",
      "E\n",
      "F\n",
      "G\n",
      "H\n",
      "I\n",
      "J\n",
      "K\n",
      "L\n",
      "M\n",
      "N\n",
      "O\n",
      "P\n",
      "Q\n",
      "R\n",
      "S\n",
      "T\n",
      "U\n",
      "V\n",
      "W\n",
      "X\n",
      "Y\n",
      "Z\n",
      "[\n",
      "\\\n",
      "]\n",
      "^\n",
      "_\n",
      "`\n",
      "a\n",
      "b\n",
      "c\n",
      "d\n",
      "e\n",
      "f\n",
      "g\n",
      "h\n",
      "i\n",
      "j\n",
      "k\n",
      "l\n",
      "m\n",
      "n\n",
      "o\n",
      "p\n",
      "q\n",
      "r\n",
      "s\n",
      "t\n",
      "u\n",
      "v\n",
      "w\n",
      "x\n",
      "y\n",
      "z\n",
      "{\n",
      "|\n",
      "}\n",
      "~\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "¡\n",
      "¢\n",
      "£\n",
      "¤\n",
      "¥\n",
      "¦\n",
      "§\n",
      "¨\n",
      "©\n",
      "ª\n",
      "«\n",
      "¬\n",
      "­\n",
      "®\n",
      "¯\n",
      "°\n",
      "±\n",
      "²\n",
      "³\n",
      "´\n",
      "µ\n",
      "¶\n",
      "·\n",
      "¸\n",
      "¹\n",
      "º\n",
      "»\n",
      "¼\n",
      "½\n",
      "¾\n",
      "¿\n",
      "À\n",
      "Á\n",
      "Â\n",
      "Ã\n",
      "Ä\n",
      "Å\n",
      "Æ\n",
      "Ç\n",
      "È\n",
      "É\n",
      "Ê\n",
      "Ë\n",
      "Ì\n",
      "Í\n",
      "Î\n",
      "Ï\n",
      "Ð\n",
      "Ñ\n",
      "Ò\n",
      "Ó\n",
      "Ô\n",
      "Õ\n",
      "Ö\n",
      "×\n",
      "Ø\n",
      "Ù\n",
      "Ú\n",
      "Û\n",
      "Ü\n",
      "Ý\n",
      "Þ\n",
      "ß\n",
      "à\n",
      "á\n",
      "â\n",
      "ã\n",
      "ä\n",
      "å\n",
      "æ\n",
      "ç\n",
      "è\n",
      "é\n",
      "ê\n",
      "ë\n",
      "ì\n",
      "í\n",
      "î\n",
      "ï\n",
      "ð\n",
      "ñ\n",
      "ò\n",
      "ó\n",
      "ô\n",
      "õ\n",
      "ö\n",
      "÷\n",
      "ø\n",
      "ù\n",
      "ú\n",
      "û\n",
      "ü\n",
      "ý\n",
      "þ\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 255):\n",
    "    print(chr(i))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
